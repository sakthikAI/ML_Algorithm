{"cells":[{"cell_type":"markdown","source":["supervised\n","\n","regression\n","\n","linear regression\n","decision tree\n","random foret\n","\n","\n","classification\n","\n","logistic regression\n","svm\n","naive bayes\n","\n","\n"],"metadata":{"id":"tlhgTpopPbxD"},"id":"tlhgTpopPbxD"},{"cell_type":"code","execution_count":null,"id":"81083574-1844-44d8-9b09-ca689912735d","metadata":{"id":"81083574-1844-44d8-9b09-ca689912735d","outputId":"fe4a853d-adf5-4b2e-b257-f9540b54f940"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>EstimatedSalary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>15624510</td>\n","      <td>Male</td>\n","      <td>19</td>\n","      <td>19000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15810944</td>\n","      <td>Male</td>\n","      <td>35</td>\n","      <td>20000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>15668575</td>\n","      <td>Female</td>\n","      <td>26</td>\n","      <td>43000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15603246</td>\n","      <td>Female</td>\n","      <td>27</td>\n","      <td>57000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>15804002</td>\n","      <td>Male</td>\n","      <td>19</td>\n","      <td>76000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>395</th>\n","      <td>15691863</td>\n","      <td>Female</td>\n","      <td>46</td>\n","      <td>41000</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>396</th>\n","      <td>15706071</td>\n","      <td>Male</td>\n","      <td>51</td>\n","      <td>23000</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>397</th>\n","      <td>15654296</td>\n","      <td>Female</td>\n","      <td>50</td>\n","      <td>20000</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>398</th>\n","      <td>15755018</td>\n","      <td>Male</td>\n","      <td>36</td>\n","      <td>33000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>399</th>\n","      <td>15594041</td>\n","      <td>Female</td>\n","      <td>49</td>\n","      <td>36000</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["      User ID  Gender  Age  EstimatedSalary  Purchased\n","0    15624510    Male   19            19000          0\n","1    15810944    Male   35            20000          0\n","2    15668575  Female   26            43000          0\n","3    15603246  Female   27            57000          0\n","4    15804002    Male   19            76000          0\n","..        ...     ...  ...              ...        ...\n","395  15691863  Female   46            41000          1\n","396  15706071    Male   51            23000          1\n","397  15654296  Female   50            20000          1\n","398  15755018    Male   36            33000          0\n","399  15594041  Female   49            36000          1\n","\n","[400 rows x 5 columns]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","data=pd.read_csv('dataset/User_Data.csv')\n","data"]},{"cell_type":"code","execution_count":null,"id":"84abcf7b-4014-4cec-943d-f1fb9e1b2406","metadata":{"id":"84abcf7b-4014-4cec-943d-f1fb9e1b2406"},"outputs":[],"source":["# nemove casing, change to lower case\n","# remove stop words\n","# remove puncation\n","# handling missing values\n","# convert text into int if therae is a text value\n","# remove empty space"]},{"cell_type":"code","execution_count":null,"id":"16467afa-c1da-45d1-9999-39bf63f8d278","metadata":{"id":"16467afa-c1da-45d1-9999-39bf63f8d278"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import LabelEncoder\n","import string\n","import pandas as pd\n","\n","# Load the text data\n","df = pd.read_csv('your_data.csv')\n","\n","# Remove casing, convert to lower case\n","df['text'] = df['text'].apply(lambda x: x.lower())\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","df['text'] = df['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n","\n","# Remove punctuation\n","df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n","\n","# Handle missing values\n","df['text'].fillna('', inplace=True)\n","\n","# Convert text into int if there's a text value (using LabelEncoder)\n","le = LabelEncoder()\n","df['text'] = le.fit_transform(df['text'])\n","\n","# Remove empty space\n","df['text'] = df['text'].apply(lambda x: x.strip())"]},{"cell_type":"markdown","id":"76d78c6d-6957-4d54-abdf-bd8deb6d26e4","metadata":{"id":"76d78c6d-6957-4d54-abdf-bd8deb6d26e4"},"source":["# Supervised learning\n","    regression\n","        linear regression\n","        decision tree\n","        random forest\n","\n","    classification\n","        Logistic Regression\n","        Support Vector Machine\n","        Naive Bayes\n","        Decision Tree, Random Forest\n","\n","# UnSupervised learning\n","    Clustering\n","    \n","Dimensionality Reductio\n","    \n","Principal Component Analysis (PCA)"]},{"cell_type":"markdown","id":"0fc266f7-1f15-4461-a385-db68c237ed95","metadata":{"id":"0fc266f7-1f15-4461-a385-db68c237ed95"},"source":["# Supervised learning"]},{"cell_type":"code","execution_count":null,"id":"ef36abf2-e383-45ba-84cc-e7bbf24f393b","metadata":{"id":"ef36abf2-e383-45ba-84cc-e7bbf24f393b"},"outputs":[],"source":["# Linear regression\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","# Load dataset\n","data = pd.read_csv('your_dataset.csv')\n","\n","# Display first few rows of the dataset\n","data.head()\n","\n","# Handling missing values\n","data = data.dropna()  # or data.fillna(value)\n","\n","# Encoding categorical variables if necessary\n","data = pd.get_dummies(data, drop_first=True)\n","\n","# Splitting features and target variable\n","X = data.drop('target_column', axis=1)\n","y = data['target_column']\n","\n","# Splitting into training and testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardizing the features for algorithms that require it\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Linear Regression\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)\n","lr_pred = lr.predict(X_test)\n","print(f'Linear Regression MSE: {mean_squared_error(y_test, lr_pred)}')"]},{"cell_type":"code","execution_count":null,"id":"c467001b-7710-42f9-8b60-0294137aff84","metadata":{"id":"c467001b-7710-42f9-8b60-0294137aff84"},"outputs":[],"source":["# DECISION TREE\n","\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics # accuray calculatare\n","\n","col_names=['pregnant','glucose','bp','skin','insulin','bmi','pedigree','age','label']\n","# load dataset\n","pima=pd.read_csv(\"dataset/pima-indians-diabetes.csv\",header=None,names=col_names)\n","pima.head()\n","\n","features_cols=['pregnant','insulin','bmi','age','glucose','bp','pedigree']\n","x=pima[features_cols]#Features\n","y=pima.label # target variable\n","x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.3,random_state=1) #70% train 30% test data\n","\n","clf=DecisionTreeClassifier()\n","clf=clf.fit(x_train,y_train)\n","y_pred=clf.predict(x_test)\n","print('accuracy:',metrics.accuracy_score(y_test,y_pred))\n","\n","clf=DecisionTreeClassifier(criterion='entropy', max_depth=3)\n","clf=clf.fit(x_train,y_train)\n","y_pred=clf.predict(x_test)\n","print('accuracy:',metrics.accuracy_score(y_test,y_pred))"]},{"cell_type":"code","execution_count":null,"id":"3fe1ea31-c7f4-4f61-9be5-d5de47fcd9dd","metadata":{"id":"3fe1ea31-c7f4-4f61-9be5-d5de47fcd9dd"},"outputs":[],"source":["# RANDOM FOREST\n","\n","from sklearn import datasets\n","# load dataset\n","iris = datasets.load_iris()\n","\n","#creating a dataframes of given iris dataset\n","\n","import pandas as pd\n","data= pd.DataFrame({\n","    'sepal length':iris.data[:,0],\n","    'sepal width': iris.data[:,1],\n","    'petal length':iris.data[:,2],\n","    'petal width':iris.data[:,3],\n","    'species':iris.target})\n","data.head()\n","\n","# import train_test_split function\n","from sklearn.model_selection import train_test_split\n","\n","x=data[['sepal length','sepal width','petal length','petal width']]\n","y=data['species'] #labels\n","x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.3)\n","\n","from sklearn.ensemble import RandomForestClassifier\n","clf=RandomForestClassifier(n_estimators=100) # we use only 100 decision trees ------- n_estimators=100\n","clf.fit(x_train,y_train)\n","y_pred=clf.predict(x_test)\n","\n","from sklearn import metrics\n","print('accuracy:',metrics.accuracy_score(y_test,y_pred))\n","\n","import pickle\n","with open('logistic_model.pkl','wb') as file: # wb--write binary\n","    pickle.dump(RandomForestClassifier,file)\n","with open('logistic_model.pkl', 'rb') as f:\n","    RandomForestClassifier=pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"468e98d5-ae21-4350-926b-4028597e60c6","metadata":{"id":"468e98d5-ae21-4350-926b-4028597e60c6"},"outputs":[],"source":["# Logestic regression\n","\n","import pandas as pd\n","import numpy as np\n","# import matplotlib.pyplot as plt\n","dataset = pd.read_csv('dataset/User_Data.csv')\n","dataset\n","\n","dataset['Gender'] = dataset['Gender'].map({'Male':0,'Female': 1, }).astype(float)\n","dataset\n","\n","x = dataset.iloc[:, [2, 3]].values\n","y = dataset.iloc[:, 4].values\n","\n","from sklearn.model_selection import train_test_split\n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n","\n","xtrain\n","\n","from sklearn.preprocessing import StandardScaler\n","sc_x = StandardScaler()\n","xtrain = sc_x.fit_transform(xtrain)\n","xtest = sc_x.transform(xtest)\n","\n","print (xtrain[0:10, :])\n","print (xtest[0:10, :])\n","\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(random_state = 0)\n","classifier.fit(xtrain, ytrain)\n","\n","y_pred = classifier.predict(xtest)\n","y_pred\n","\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(ytest, y_pred)\n","print (\"Confusion Matrix : \\n\", cm)\n","\n","from sklearn.metrics import accuracy_score\n","print (\"Accuracy : \", accuracy_score(ytest, y_pred))\n","\n","from sklearn.metrics import precision_score\n","print (\"Precision : \", precision_score(ytest, y_pred))"]},{"cell_type":"code","execution_count":null,"id":"2b30c16e-32ac-4f5b-a06b-8198e00e869e","metadata":{"id":"2b30c16e-32ac-4f5b-a06b-8198e00e869e"},"outputs":[],"source":["# SVM\n","\n","#Data Pre-processing Step\n","# importing libraries\n","import numpy as nm\n","import matplotlib.pyplot as mtp\n","import pandas as pd\n","\n","#importing datasets\n","data_set= pd.read_csv('dataset/User_Data.csv')\n","\n","#Extracting Independent and dependent Variable\n","x= data_set.iloc[:, [2,3]].values\n","y= data_set.iloc[:, 4].values\n","\n","# Splitting the dataset into training and test set.\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)\n","#feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","st_x= StandardScaler()\n","x_train= st_x.fit_transform(x_train)\n","x_test= st_x.transform(x_test)\n","\n","from sklearn.svm import SVC # \"Support vector classifier\"\n","classifier = SVC(kernel='linear', random_state=0)\n","classifier.fit(x_train, y_train)\n","\n","#Predicting the test set result\n","y_pred= classifier.predict(x_test)\n","\n","#Creating the Confusion matrix\n","from sklearn.metrics import confusion_matrix,precision_score,recall_score\n","cm= confusion_matrix(y_test, y_pred)\n","\n","from sklearn.metrics import accuracy_score\n","\n","accuracy_score(y_test,y_pred)\n","\n","precision_score(y_test, y_pred)\n","\n","recall_score(y_test,y_pred)"]},{"cell_type":"code","execution_count":null,"id":"38bb7059-7d9d-4883-b3a9-30a611da3bd4","metadata":{"id":"38bb7059-7d9d-4883-b3a9-30a611da3bd4"},"outputs":[],"source":["# Naive bayes  ---- sentiment analysis based on high biased  - text based data\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","data=pd.read_csv('dataset/app_review.csv')\n","data.head()\n","\n","def preprocess_data(data):\n","    # remove package anme as it's not relevent\n","    data=data.drop('package_name',axis=1)\n","    # convert text to lowercase\n","    data['review']=data['review'].str.strip().str.lower() #strip()---remove white space\n","    return data\n","data=preprocess_data(data)\n","data.head()\n","\n","# split into training and testing data\n","x=data['review']#train data\n","y=data['polarity'] # labels\n","x,x_test,y,y_test= train_test_split(x,y,stratify=y,test_size=0.25,random_state=42)\n","\n","vec =CountVectorizer(stop_words='english')\n","x= vec.fit_transform(x).toarray()\n","x_test=vec.transform(x_test).toarray()\n","\n","from sklearn.naive_bayes import MultinomialNB\n","model=MultinomialNB()\n","model.fit(x,y)\n","\n","model.score(x_test,y_test)\n","\n","model.predict(vec.transform(['Love this app simply awesome!']))\n","\n","model.predict(vec.transform(['Its getting stuck alot while using the app']))"]},{"cell_type":"code","execution_count":null,"id":"84a37df6-606e-412a-94f6-07370e693af6","metadata":{"id":"84a37df6-606e-412a-94f6-07370e693af6"},"outputs":[],"source":["# KNN\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#importing dataset\n","data=pd.read_csv('dataset/User_Data.csv')\n","\n","#extracting independent and dependent variable\n","x=data.iloc[:,[2,3]].values\n","y=data.iloc[:,4].values\n","print(x[:10])\n","\n","# splitting dataset into train and test set\n","from sklearn.model_selection import train_test_split\n","xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=0)\n","\n","#feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc_x=StandardScaler()\n","xtrain=sc_x.fit_transform(xtrain)\n","xtest=sc_x.transform(xtest)\n","print(xtrain[:10])\n","\n","#fitting KNN classifier to the training set\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","classifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n","classifier.fit(xtrain,ytrain)\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","classifier=KNeighborsClassifier(n_neighbors=5,metric='manhattan',p=2)\n","classifier.fit(xtrain,ytrain)\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","classifier=KNeighborsClassifier(n_neighbors=5,metric='euclidean',p=2)\n","classifier.fit(xtrain,ytrain)\n","\n","#predicting the test set result\n","y_pred=classifier.predict(xtest)\n","y_pred\n","\n","#creating the confusion matrix\n","from sklearn.metrics import confusion_matrix\n","cm= confusion_matrix(ytest,y_pred)\n","print(\"confusion matrix:\\n\",cm)\n","\n","from sklearn.metrics import accuracy_score\n","print(\"accuracy:\",accuracy_score(ytest,y_pred))\n","\n","from sklearn.metrics import precision_score\n","print(\"precision:\",precision_score(ytest,y_pred))"]},{"cell_type":"markdown","id":"aab5b35e-7203-4bdc-9931-bd1ef49e70bf","metadata":{"id":"aab5b35e-7203-4bdc-9931-bd1ef49e70bf"},"source":["# Unsupervised learning"]},{"cell_type":"code","execution_count":null,"id":"b136711b-e508-488c-87b5-5f8d70e13a60","metadata":{"id":"b136711b-e508-488c-87b5-5f8d70e13a60"},"outputs":[],"source":["# KMeans cluster\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","%matplotlib inline\n","\n","# generating two cluster and plotting\n","x= -2*np.random.rand(100,2) # random.rand== get random number, 100 * 2 --- 100 array with 2 values\n","print(x)\n","x1=1+2*np.random.rand(50,2)\n","print(x1)\n","x[50:100,:]=x1\n","print(x)\n","plt.scatter(x[:,0],x[:,1],s=50,c='b')\n","plt.show()\n","# x[:,0],x[:,1]--- oth index and 1st index ---x,y\n","\n","Kmean=KMeans(n_clusters=2)\n","Kmean.fit(x)\n","\n","# centroid --- the centre of the data points of the specific clusters\n","Kmean.cluster_centers_\n","\n","plt.scatter(x[:,0],x[:,1],s=50,c='b')\n","plt.scatter(2.1171602 ,  2.13394506,s=200,c='g', marker='d')\n","plt.scatter(-1.02601352, -0.89138649,s=200,c='r', marker='s')\n","plt.show()\n","\n","Kmean.labels_\n","\n","# need to check where thsese data points in the cluster---3.0,3.0\n","\n","sample_test=np.array([3.0,3.0])\n","second_test=sample_test.reshape(1,-1)\n","print(second_test)\n","Kmean.predict(second_test)\n","\n","sample_test=np.array([-3.0,3.0])\n","second_test=sample_test.reshape(1,-1)\n","print(second_test)\n","Kmean.predict(second_test)"]},{"cell_type":"code","execution_count":null,"id":"61207ee6-6f27-46c3-ae55-72fca3cb3e3a","metadata":{"id":"61207ee6-6f27-46c3-ae55-72fca3cb3e3a"},"outputs":[],"source":["# Hierarchal clustering\n","\n","import numpy as np\n","\n","X=np.array([[5,3],\n","            [10,15],\n","            [15,12],\n","            [24,10],\n","            [30,30],\n","            [85,70],\n","            [71,80],\n","            [60,78],\n","            [70,55],\n","            [80,91],])\n","\n","import matplotlib.pyplot as plt\n","\n","labels =range(1,11)\n","\n","plt.figure(figsize=(10,7))\n","plt.subplots_adjust(bottom=0.1)\n","plt.scatter(X[:,0], X[:,1],label='True position')\n","\n","\n","for label, x, y, in zip(labels,X[:,0],X[:,1]): # x[:,0] -- all rows and oth column, x[:,1]--- all rows and 1st column\n","    plt.annotate(\n","        label,\n","        xy=(x,y),xytext=(-3,3),\n","        textcoords='offset points', ha='right',va='bottom') # xytext=(-3,3)---position of the text\n","plt.show()\n","\n","from scipy.cluster.hierarchy import dendrogram,linkage\n","#import matplotlib.pyplot as plt\n","from matplotlib import pyplot as plt\n","linked= linkage(X,'single')\n","labelList=range(1,11)\n","plt.figure(figsize=(10,7))\n","dendrogram(linked,\n","           orientation='top',\n","           labels=labelList,\n","           distance_sort='descending',\n","           show_leaf_counts=True)\n","plt.show()\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.cluster import AgglomerativeClustering\n","\n","# if we don't know the position of datapoints or cluster so that we use matplotlib to plot a graph\n","\n","X=np.array([[5,3],\n","            [10,15],\n","            [15,12],\n","            [24,10],\n","            [30,30],\n","            [85,70],\n","            [71,80],\n","            [60,78],\n","            [70,55],\n","            [80,91],])\n","\n","from sklearn.cluster import AgglomerativeClustering\n","cluster=AgglomerativeClustering(n_clusters=2,metric='euclidean',linkage='ward')\n","cluster.fit_predict(X)\n","#default metric euclidean\n","\n","print(cluster.labels_)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}